{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fff5ac-b2e0-4346-8377-f76c139e35cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 14:40:22,046 - INFO - üöÄ Starting Market Screener for Grid Trading Pairs\n",
      "2025-09-13 14:40:22,048 - INFO - ============================================================\n",
      "2025-09-13 14:40:22,051 - WARNING - ‚ö†Ô∏è  api_config.py not found, running without API credentials\n",
      "2025-09-13 14:40:22,051 - INFO - üåê Using public endpoints only\n",
      "2025-09-13 14:40:24,367 - INFO - Found 419 active USDT spot pairs\n",
      "2025-09-13 14:40:24,514 - INFO - Sorted pairs by trading volume\n",
      "2025-09-13 14:40:24,515 - INFO - Limited screening to top 50 most liquid pairs\n",
      "2025-09-13 14:40:24,515 - INFO - Starting screening of 50 pairs with 3 workers...\n",
      "2025-09-13 14:40:24,516 - INFO - Screening USDC/USDT...\n",
      "2025-09-13 14:40:24,519 - INFO - Screening ETH/USDT...\n",
      "2025-09-13 14:40:24,519 - INFO - Screening BTC/USDT...\n",
      "2025-09-13 14:40:24,660 - INFO - Screening SOL/USDT...\n",
      "2025-09-13 14:40:24,969 - INFO - Fetched 365 candles for SOL/USDT\n",
      "2025-09-13 14:40:24,974 - INFO - Screening DOGE/USDT...\n",
      "2025-09-13 14:40:25,069 - INFO - Fetched 365 candles for ETH/USDT\n",
      "2025-09-13 14:40:25,072 - INFO - Screening XRP/USDT...\n",
      "2025-09-13 14:40:25,074 - INFO - Fetched 365 candles for BTC/USDT\n",
      "2025-09-13 14:40:25,076 - INFO - Screening BNB/USDT...\n",
      "2025-09-13 14:40:25,114 - INFO - Screening ADA/USDT...\n",
      "2025-09-13 14:40:25,259 - INFO - Screening LINK/USDT...\n",
      "2025-09-13 14:40:25,420 - INFO - Fetched 365 candles for BNB/USDT\n",
      "2025-09-13 14:40:25,423 - INFO - Screening TRX/USDT...\n",
      "2025-09-13 14:40:25,460 - INFO - Fetched 365 candles for XRP/USDT\n",
      "2025-09-13 14:40:25,463 - INFO - Screening LTC/USDT...\n",
      "2025-09-13 14:40:25,573 - INFO - Screening DOT/USDT...\n",
      "2025-09-13 14:40:25,594 - INFO - Fetched 365 candles for LINK/USDT\n",
      "2025-09-13 14:40:25,597 - INFO - Screening HBAR/USDT...\n",
      "2025-09-13 14:40:25,598 - INFO - Progress: 10/50 pairs processed\n",
      "2025-09-13 14:40:25,737 - INFO - Screening BCH/USDT...\n",
      "2025-09-13 14:40:25,801 - INFO - Fetched 365 candles for LTC/USDT\n",
      "2025-09-13 14:40:25,803 - INFO - Screening XLM/USDT...\n",
      "2025-09-13 14:40:25,934 - INFO - Fetched 365 candles for DOT/USDT\n",
      "2025-09-13 14:40:25,937 - INFO - Screening CRV/USDT...\n",
      "2025-09-13 14:40:25,945 - INFO - Screening EUR/USDT...\n",
      "2025-09-13 14:40:26,050 - INFO - Fetched 365 candles for BCH/USDT\n",
      "2025-09-13 14:40:26,053 - INFO - Screening ETC/USDT...\n",
      "2025-09-13 14:40:26,214 - INFO - Screening FET/USDT...\n",
      "2025-09-13 14:40:26,336 - INFO - Fetched 365 candles for EUR/USDT\n",
      "2025-09-13 14:40:26,339 - INFO - Screening PAXG/USDT...\n",
      "2025-09-13 14:40:26,353 - INFO - Fetched 365 candles for ETC/USDT\n",
      "2025-09-13 14:40:26,354 - INFO - Screening SAND/USDT...\n",
      "2025-09-13 14:40:26,355 - INFO - Screening ATOM/USDT...\n",
      "2025-09-13 14:40:26,504 - INFO - Screening TRB/USDT...\n",
      "2025-09-13 14:40:26,504 - INFO - Progress: 20/50 pairs processed\n",
      "2025-09-13 14:40:26,691 - INFO - Fetched 365 candles for ATOM/USDT\n",
      "2025-09-13 14:40:26,693 - INFO - Screening NMR/USDT...\n",
      "2025-09-13 14:40:26,726 - INFO - Fetched 365 candles for PAXG/USDT\n",
      "2025-09-13 14:40:26,728 - INFO - ‚úÖ PAXG/USDT passed all filters: Price=$3631.4800, Hurst=0.4996, 7d Volume=$123,444,288\n",
      "2025-09-13 14:40:26,729 - INFO - Screening COMP/USDT...\n",
      "2025-09-13 14:40:26,729 - INFO - Progress: 22/50 - Found 1 valid pairs\n",
      "2025-09-13 14:40:26,822 - INFO - Fetched 365 candles for TRB/USDT\n",
      "2025-09-13 14:40:26,824 - INFO - Screening VET/USDT...\n",
      "2025-09-13 14:40:26,964 - INFO - Screening ALGO/USDT...\n",
      "2025-09-13 14:40:27,055 - INFO - Fetched 365 candles for COMP/USDT\n",
      "2025-09-13 14:40:27,058 - INFO - Screening UMA/USDT...\n",
      "2025-09-13 14:40:27,110 - INFO - Screening RUNE/USDT...\n",
      "2025-09-13 14:40:27,236 - INFO - Fetched 365 candles for NMR/USDT\n",
      "2025-09-13 14:40:27,238 - INFO - Screening XTZ/USDT...\n",
      "2025-09-13 14:40:27,378 - INFO - Screening NEO/USDT...\n",
      "2025-09-13 14:40:27,429 - INFO - Fetched 365 candles for RUNE/USDT\n",
      "2025-09-13 14:40:27,437 - INFO - Screening CTSI/USDT...\n",
      "2025-09-13 14:40:27,446 - INFO - Fetched 365 candles for UMA/USDT\n",
      "2025-09-13 14:40:27,448 - INFO - Screening MANA/USDT...\n",
      "2025-09-13 14:40:27,448 - INFO - Progress: 30/50 pairs processed\n",
      "2025-09-13 14:40:27,588 - INFO - Screening SUSHI/USDT...\n",
      "2025-09-13 14:40:27,609 - INFO - Screening QTUM/USDT...\n",
      "2025-09-13 14:40:27,697 - INFO - Fetched 365 candles for NEO/USDT\n",
      "2025-09-13 14:40:27,700 - INFO - Screening THETA/USDT...\n",
      "2025-09-13 14:40:27,732 - INFO - Screening IOTA/USDT...\n",
      "2025-09-13 14:40:27,839 - INFO - Screening MKR/USDT...\n",
      "2025-09-13 14:40:27,872 - INFO - Screening MDT/USDT...\n",
      "2025-09-13 14:40:27,985 - INFO - Fetched 365 candles for QTUM/USDT\n",
      "2025-09-13 14:40:27,988 - INFO - Screening RSR/USDT...\n",
      "2025-09-13 14:40:28,019 - INFO - Screening ZEC/USDT...\n",
      "2025-09-13 14:40:28,145 - INFO - Screening ZEN/USDT...\n",
      "2025-09-13 14:40:28,223 - INFO - Fetched 365 candles for MKR/USDT\n",
      "2025-09-13 14:40:28,225 - INFO - Screening CHZ/USDT...\n",
      "2025-09-13 14:40:28,225 - INFO - Progress: 40/50 pairs processed\n",
      "2025-09-13 14:40:28,366 - INFO - Screening RLC/USDT...\n",
      "2025-09-13 14:40:28,437 - INFO - Fetched 365 candles for ZEN/USDT\n",
      "2025-09-13 14:40:28,439 - INFO - Screening STX/USDT...\n",
      "2025-09-13 14:40:28,480 - INFO - Fetched 365 candles for ZEC/USDT\n",
      "2025-09-13 14:40:28,482 - INFO - Screening EGLD/USDT...\n",
      "2025-09-13 14:40:28,585 - INFO - Screening ZRX/USDT...\n",
      "2025-09-13 14:40:28,724 - INFO - Screening ONE/USDT...\n",
      "2025-09-13 14:40:28,794 - INFO - Fetched 365 candles for EGLD/USDT\n",
      "2025-09-13 14:40:28,796 - INFO - Screening NKN/USDT...\n",
      "2025-09-13 14:40:28,870 - INFO - Screening KAVA/USDT...\n",
      "2025-09-13 14:40:28,885 - INFO - Fetched 365 candles for RLC/USDT\n",
      "2025-09-13 14:40:29,010 - INFO - Progress: 50/50 pairs processed\n",
      "2025-09-13 14:40:29,011 - INFO - Screening completed. 1 pairs passed filters.\n",
      "2025-09-13 14:40:29,011 - INFO - Top 1 pairs selected.\n",
      "2025-09-13 14:40:29,012 - INFO - \n",
      "‚è±Ô∏è  Screening completed in 6.95 seconds\n",
      "2025-09-13 14:40:29,012 - INFO - üìä Found 1 pairs that passed all criteria\n",
      "2025-09-13 14:40:29,013 - INFO - \n",
      "üèÜ TOP PAIRS FOR GRID TRADING:\n",
      "2025-09-13 14:40:29,013 - INFO - ------------------------------------------------------------\n",
      "2025-09-13 14:40:29,013 - INFO - 1. PAXG/USDT\n",
      "2025-09-13 14:40:29,014 - INFO -    üí∞ Price: $3631.4800\n",
      "2025-09-13 14:40:29,014 - INFO -    üìà Hurst: 0.4996\n",
      "2025-09-13 14:40:29,015 - INFO -    üíµ 7d Volume: $123,444,288\n",
      "2025-09-13 14:40:29,015 - INFO -    üìä 7d Volume (base): 33,954.03\n",
      "2025-09-13 14:40:29,015 - INFO - \n",
      "2025-09-13 14:40:29,017 - INFO - Results saved to screening_results.json\n",
      "2025-09-13 14:40:29,018 - INFO - üìã SCREENING SUMMARY:\n",
      "2025-09-13 14:40:29,018 - INFO -    Min Price: $1.0\n",
      "2025-09-13 14:40:29,019 - INFO -    Min 7d Volume: $10,000,000\n",
      "2025-09-13 14:40:29,019 - INFO -    Max Hurst: 0.5\n",
      "2025-09-13 14:40:29,020 - INFO -    Data Period: 1 years\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Market Screener for Grid Trading Pairs\n",
    "Fetches all spot/USDT pairs from Binance and applies screening criteria\n",
    "\"\"\"\n",
    "\n",
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class PairData:\n",
    "    \"\"\"Data structure for trading pair information\"\"\"\n",
    "    symbol: str\n",
    "    close_price: float\n",
    "    hurst_exponent: float\n",
    "    volume_7d: float\n",
    "    volume_7d_usd: float\n",
    "    last_update: datetime\n",
    "    ohlcv_data: Optional[pd.DataFrame] = None\n",
    "\n",
    "@dataclass\n",
    "class ScreeningCriteria:\n",
    "    \"\"\"Screening criteria configuration with validation\"\"\"\n",
    "    min_price: float = 1.0\n",
    "    min_volume_7d_usd: float = 10_000_000\n",
    "    max_hurst: float = 0.5  # Hurst < 0.5 for mean-reverting\n",
    "    years_of_data: int = 1  # 1 year of data\n",
    "    top_n_pairs: int = 5  # Top 5 pairs\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate criteria values\"\"\"\n",
    "        if self.min_price <= 0:\n",
    "            raise ValueError(\"min_price must be positive\")\n",
    "        if self.min_volume_7d_usd <= 0:\n",
    "            raise ValueError(\"min_volume_7d_usd must be positive\")\n",
    "        if not 0 <= self.max_hurst <= 1:\n",
    "            raise ValueError(\"max_hurst must be between 0 and 1\")\n",
    "        if self.years_of_data <= 0:\n",
    "            raise ValueError(\"years_of_data must be positive\")\n",
    "        if self.top_n_pairs <= 0:\n",
    "            raise ValueError(\"top_n_pairs must be positive\")\n",
    "\n",
    "class HurstCalculator:\n",
    "    \"\"\"Calculate Hurst exponent for time series analysis with robust error handling\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_hurst(price_series: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Hurst exponent using R/S analysis\n",
    "        \n",
    "        Args:\n",
    "            price_series: Price series (typically close prices)\n",
    "            \n",
    "        Returns:\n",
    "            Hurst exponent value (0-1, where <0.5 = mean-reverting, >0.5 = trending)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Input validation\n",
    "            if price_series is None or price_series.empty:\n",
    "                logger.warning(\"Empty price series provided for Hurst calculation\")\n",
    "                return 0.5\n",
    "            \n",
    "            if len(price_series) < 20:  # Need at least 20 data points\n",
    "                logger.warning(f\"Insufficient data for Hurst calculation: {len(price_series)} points\")\n",
    "                return 0.5\n",
    "            \n",
    "            # Remove any NaN or infinite values\n",
    "            clean_series = price_series.dropna()\n",
    "            if len(clean_series) < 20:\n",
    "                logger.warning(\"Insufficient clean data after removing NaN values\")\n",
    "                return 0.5\n",
    "            \n",
    "            # Calculate log returns\n",
    "            log_returns = np.log(clean_series / clean_series.shift(1)).dropna()\n",
    "            \n",
    "            if len(log_returns) < 20:\n",
    "                logger.warning(\"Insufficient log returns for Hurst calculation\")\n",
    "                return 0.5\n",
    "            \n",
    "            # Check for constant values (no variance)\n",
    "            if log_returns.std() == 0:\n",
    "                logger.warning(\"No variance in log returns, returning neutral Hurst\")\n",
    "                return 0.5\n",
    "            \n",
    "            # R/S analysis\n",
    "            n = len(log_returns)\n",
    "            mean_return = log_returns.mean()\n",
    "            \n",
    "            # Calculate cumulative deviations\n",
    "            cumsum_deviations = (log_returns - mean_return).cumsum()\n",
    "            \n",
    "            # Calculate range R\n",
    "            R = cumsum_deviations.max() - cumsum_deviations.min()\n",
    "            \n",
    "            # Calculate standard deviation S\n",
    "            S = log_returns.std()\n",
    "            \n",
    "            if S == 0 or R == 0:\n",
    "                logger.warning(\"Zero variance or range in R/S calculation\")\n",
    "                return 0.5\n",
    "            \n",
    "            # R/S ratio\n",
    "            rs_ratio = R / S\n",
    "            \n",
    "            if rs_ratio <= 0:\n",
    "                logger.warning(\"Invalid R/S ratio, returning neutral Hurst\")\n",
    "                return 0.5\n",
    "            \n",
    "            # Hurst exponent calculation\n",
    "            hurst = np.log(rs_ratio) / np.log(n)\n",
    "            \n",
    "            # Clamp between 0 and 1\n",
    "            hurst = max(0.0, min(1.0, hurst))\n",
    "            \n",
    "            return float(hurst)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating Hurst exponent: {e}\")\n",
    "            return 0.5  # Return neutral value on error\n",
    "\n",
    "class BinanceDataFetcher:\n",
    "    \"\"\"Enhanced data fetcher for Binance with error handling and caching\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None, api_secret: Optional[str] = None):\n",
    "        # Configure exchange with API credentials if available\n",
    "        exchange_config = {\n",
    "            \"enableRateLimit\": True,\n",
    "            \"options\": {\"defaultType\": \"spot\"}  # Make sure it's spot, not futures\n",
    "        }\n",
    "        \n",
    "        # Add API credentials if provided\n",
    "        if api_key and api_secret:\n",
    "            exchange_config.update({\n",
    "                \"apiKey\": api_key,\n",
    "                \"secret\": api_secret\n",
    "            })\n",
    "            logger.info(\"üîë Using API credentials for enhanced data access\")\n",
    "        else:\n",
    "            logger.info(\"üåê Using public endpoints only\")\n",
    "            \n",
    "        self.exchange = ccxt.binance(exchange_config)\n",
    "        self.cache_dir = Path('data_cache')\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def get_all_spot_usdt_pairs(self) -> List[str]:\n",
    "        \"\"\"Get all spot USDT trading pairs from Binance with API access\"\"\"\n",
    "        try:\n",
    "            # Load markets to get comprehensive list\n",
    "            markets = self.exchange.load_markets()\n",
    "            usdt_pairs = []\n",
    "            \n",
    "            for symbol, market in markets.items():\n",
    "                if (market.get('type') == 'spot' and \n",
    "                    market.get('quote') == 'USDT' and \n",
    "                    market.get('active', False)):\n",
    "                    usdt_pairs.append(symbol)\n",
    "            \n",
    "            if not usdt_pairs:\n",
    "                logger.warning(\"No USDT pairs found, using fallback list\")\n",
    "                return self._get_fallback_pairs()\n",
    "            \n",
    "            logger.info(f\"Found {len(usdt_pairs)} active USDT spot pairs\")\n",
    "            \n",
    "            # Sort by volume for better prioritization (only if we have API access)\n",
    "            try:\n",
    "                # Limit to top 100 pairs to avoid rate limits\n",
    "                top_pairs = usdt_pairs[:100]\n",
    "                tickers = self.exchange.fetch_tickers(top_pairs)\n",
    "                usdt_pairs.sort(key=lambda x: tickers.get(x, {}).get('quoteVolume', 0), reverse=True)\n",
    "                logger.info(\"Sorted pairs by trading volume\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not sort by volume: {e}\")\n",
    "            \n",
    "            return usdt_pairs\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching spot pairs: {e}\")\n",
    "            return self._get_fallback_pairs()\n",
    "    \n",
    "    def _get_fallback_pairs(self) -> List[str]:\n",
    "        \"\"\"Get fallback list of major USDT pairs\"\"\"\n",
    "        logger.info(\"Using fallback list of major USDT pairs\")\n",
    "        return [\n",
    "            'BTC/USDT', 'ETH/USDT', 'BNB/USDT', 'ADA/USDT', 'XRP/USDT',\n",
    "            'SOL/USDT', 'DOT/USDT', 'DOGE/USDT', 'AVAX/USDT', 'SHIB/USDT',\n",
    "            'MATIC/USDT', 'LTC/USDT', 'UNI/USDT', 'LINK/USDT', 'ATOM/USDT',\n",
    "            'XLM/USDT', 'BCH/USDT', 'FIL/USDT', 'TRX/USDT', 'ETC/USDT',\n",
    "            'VET/USDT', 'ICP/USDT', 'THETA/USDT', 'FTM/USDT', 'ALGO/USDT',\n",
    "            'MANA/USDT', 'SAND/USDT', 'AXS/USDT', 'CRV/USDT', 'COMP/USDT',\n",
    "            'MKR/USDT', 'SNX/USDT', 'YFI/USDT', 'AAVE/USDT', 'SUSHI/USDT'\n",
    "        ]\n",
    "    \n",
    "    def fetch_ohlcv_data(self, symbol: str, timeframe: str = '1d', \n",
    "                        years: int = 1) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Fetch OHLCV data for a symbol with caching and robust error handling\"\"\"\n",
    "        try:\n",
    "            # Input validation\n",
    "            if not symbol or not isinstance(symbol, str):\n",
    "                logger.error(f\"Invalid symbol: {symbol}\")\n",
    "                return None\n",
    "            \n",
    "            if years <= 0 or years > 5:  # Limit to reasonable range\n",
    "                logger.warning(f\"Invalid years parameter: {years}, using 1 year\")\n",
    "                years = 1\n",
    "            \n",
    "            # Check cache first\n",
    "            cache_file = self.cache_dir / f\"{symbol.replace('/', '_')}_{timeframe}_{years}y.json\"\n",
    "            \n",
    "            if cache_file.exists():\n",
    "                cache_time = datetime.fromtimestamp(cache_file.stat().st_mtime)\n",
    "                if datetime.now() - cache_time < timedelta(hours=1):  # Cache valid for 1 hour\n",
    "                    logger.info(f\"Loading {symbol} from cache\")\n",
    "                    try:\n",
    "                        with open(cache_file, 'r') as f:\n",
    "                            data = json.load(f)\n",
    "                            df = pd.DataFrame(data)\n",
    "                            if not df.empty:\n",
    "                                return df\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error loading cache for {symbol}: {e}\")\n",
    "            \n",
    "            # Calculate start date for historical data\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=years * 365)\n",
    "            \n",
    "            # Convert to timestamp\n",
    "            from_ts = self.exchange.parse8601(start_date.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            \n",
    "            # Fetch data with pagination\n",
    "            ohlcv_list = self._fetch_ohlcv_paginated(symbol, timeframe, from_ts)\n",
    "            \n",
    "            if not ohlcv_list:\n",
    "                logger.warning(f\"No data fetched for {symbol}\")\n",
    "                return None\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(ohlcv_list, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "            df = df.sort_values('timestamp').drop_duplicates(subset=['timestamp'])\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "            # Validate data quality\n",
    "            if len(df) < 30:  # Need at least 30 days\n",
    "                logger.warning(f\"Insufficient data for {symbol}: {len(df)} days\")\n",
    "                return None\n",
    "            \n",
    "            # Cache the data\n",
    "            try:\n",
    "                with open(cache_file, 'w') as f:\n",
    "                    json.dump(df.to_dict('records'), f, default=str)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error caching data for {symbol}: {e}\")\n",
    "            \n",
    "            logger.info(f\"Fetched {len(df)} candles for {symbol}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching OHLCV for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _fetch_ohlcv_paginated(self, symbol: str, timeframe: str, from_ts: int) -> List:\n",
    "        \"\"\"Fetch OHLCV data with pagination and rate limiting\"\"\"\n",
    "        ohlcv_list = []\n",
    "        max_candles = 1000  # Reasonable limit for 1 year of daily data\n",
    "        \n",
    "        try:\n",
    "            # First request\n",
    "            ohlcv = self.exchange.fetch_ohlcv(symbol, timeframe, since=from_ts, limit=1000)\n",
    "            if ohlcv:\n",
    "                ohlcv_list.extend(ohlcv)\n",
    "            \n",
    "            # Pagination loop\n",
    "            while len(ohlcv_list) < max_candles:\n",
    "                if not ohlcv or len(ohlcv) < 1000:\n",
    "                    break\n",
    "                \n",
    "                # Get the last timestamp and request next batch\n",
    "                from_ts = ohlcv[-1][0] + 1  # Add 1ms to avoid duplicates\n",
    "                ohlcv = self.exchange.fetch_ohlcv(symbol, timeframe, since=from_ts, limit=1000)\n",
    "                \n",
    "                if not ohlcv or len(ohlcv) == 0:\n",
    "                    break\n",
    "                \n",
    "                ohlcv_list.extend(ohlcv)\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "                # Safety check\n",
    "                if len(ohlcv_list) > max_candles:\n",
    "                    ohlcv_list = ohlcv_list[:max_candles]\n",
    "                    break\n",
    "            \n",
    "            return ohlcv_list\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in pagination for {symbol}: {e}\")\n",
    "            return ohlcv_list\n",
    "    \n",
    "    def get_current_ticker_data(self, symbol: str) -> Optional[Dict]:\n",
    "        \"\"\"Get current ticker data for a symbol using the exact pattern from your example\"\"\"\n",
    "        try:\n",
    "            ticker = self.exchange.fetch_ticker(symbol)\n",
    "            return ticker\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error fetching ticker for {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_volume_7d_usd(self, df: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate 7-day trading volume in USD with validation\"\"\"\n",
    "        try:\n",
    "            if df is None or df.empty:\n",
    "                logger.warning(\"Empty DataFrame provided for volume calculation\")\n",
    "                return 0.0\n",
    "            \n",
    "            if len(df) < 7:\n",
    "                logger.warning(f\"Insufficient data for 7d volume calculation: {len(df)} days\")\n",
    "                return 0.0\n",
    "            \n",
    "            # Validate required columns\n",
    "            required_cols = ['volume', 'close']\n",
    "            if not all(col in df.columns for col in required_cols):\n",
    "                logger.error(f\"Missing required columns: {required_cols}\")\n",
    "                return 0.0\n",
    "            \n",
    "            # Get last 7 days of data\n",
    "            last_7_days = df.tail(7)\n",
    "            \n",
    "            # Check for valid data\n",
    "            if last_7_days['volume'].isna().any() or last_7_days['close'].isna().any():\n",
    "                logger.warning(\"NaN values found in volume or close data\")\n",
    "                return 0.0\n",
    "            \n",
    "            # Calculate volume in USD (volume * close_price)\n",
    "            volume_usd = (last_7_days['volume'] * last_7_days['close']).sum()\n",
    "            \n",
    "            # Validate result\n",
    "            if volume_usd < 0:\n",
    "                logger.warning(\"Negative volume calculated, returning 0\")\n",
    "                return 0.0\n",
    "            \n",
    "            return float(volume_usd)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating 7d volume: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "class MarketScreener:\n",
    "    \"\"\"Main market screener class - clean and simple\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None, api_secret: Optional[str] = None):\n",
    "        self.data_fetcher = BinanceDataFetcher(api_key, api_secret)\n",
    "        self.hurst_calculator = HurstCalculator()\n",
    "        self.criteria = ScreeningCriteria()\n",
    "        self.results: List[PairData] = []\n",
    "        \n",
    "    def screen_single_pair(self, symbol: str) -> Optional[PairData]:\n",
    "        \"\"\"Screen a single trading pair with comprehensive validation and error handling\"\"\"\n",
    "        try:\n",
    "            # Input validation\n",
    "            if not symbol or not isinstance(symbol, str):\n",
    "                logger.error(f\"Invalid symbol: {symbol}\")\n",
    "                return None\n",
    "            \n",
    "            logger.info(f\"Screening {symbol}...\")\n",
    "            \n",
    "            # Step 1: Get current ticker data for quick filters\n",
    "            ticker_data = self.data_fetcher.get_current_ticker_data(symbol)\n",
    "            if not ticker_data:\n",
    "                logger.warning(f\"No ticker data available for {symbol}\")\n",
    "                return None\n",
    "            \n",
    "            # Step 2: Price filter (quick)\n",
    "            current_price = ticker_data.get('last', 0)\n",
    "            if current_price <= 0:\n",
    "                logger.warning(f\"Invalid price for {symbol}: {current_price}\")\n",
    "                return None\n",
    "            \n",
    "            if current_price < self.criteria.min_price:\n",
    "                logger.debug(f\"{symbol} filtered out: price ${current_price:.4f} < ${self.criteria.min_price}\")\n",
    "                return None\n",
    "            \n",
    "            # Step 3: Volume pre-filter (quick)\n",
    "            volume_24h = ticker_data.get('quoteVolume', 0)\n",
    "            if volume_24h <= 0:\n",
    "                logger.warning(f\"Invalid volume for {symbol}: {volume_24h}\")\n",
    "                return None\n",
    "            \n",
    "            # Quick volume check (24h volume * 7 should be roughly 7d volume)\n",
    "            estimated_7d_volume = volume_24h * 7\n",
    "            if estimated_7d_volume < self.criteria.min_volume_7d_usd * 0.5:  # Conservative estimate\n",
    "                logger.debug(f\"{symbol} filtered out: estimated 7d volume ${estimated_7d_volume:,.0f} too low\")\n",
    "                return None\n",
    "            \n",
    "            # Step 4: Fetch OHLCV data for detailed analysis\n",
    "            df = self.data_fetcher.fetch_ohlcv_data(symbol, years=self.criteria.years_of_data)\n",
    "            if df is None or df.empty:\n",
    "                logger.warning(f\"No OHLCV data available for {symbol}\")\n",
    "                return None\n",
    "            \n",
    "            # Step 5: Data quality check\n",
    "            if len(df) < 30:  # At least 30 days of data\n",
    "                logger.warning(f\"Insufficient data for {symbol}: {len(df)} days\")\n",
    "                return None\n",
    "            \n",
    "            # Step 6: Calculate 7-day volume in USD (detailed check)\n",
    "            volume_7d_usd = self.data_fetcher.calculate_volume_7d_usd(df)\n",
    "            if volume_7d_usd < self.criteria.min_volume_7d_usd:\n",
    "                logger.debug(f\"{symbol} filtered out: 7d volume ${volume_7d_usd:,.0f} < ${self.criteria.min_volume_7d_usd:,.0f}\")\n",
    "                return None\n",
    "            \n",
    "            # Step 7: Calculate Hurst exponent\n",
    "            hurst = self.hurst_calculator.calculate_hurst(df['close'])\n",
    "            if hurst >= self.criteria.max_hurst:\n",
    "                logger.debug(f\"{symbol} filtered out: Hurst {hurst:.4f} >= {self.criteria.max_hurst}\")\n",
    "                return None\n",
    "            \n",
    "            # Step 8: Calculate 7-day volume in base currency\n",
    "            volume_7d = df.tail(7)['volume'].sum()\n",
    "            \n",
    "            # Step 9: Create pair data\n",
    "            pair_data = PairData(\n",
    "                symbol=symbol,\n",
    "                close_price=current_price,\n",
    "                hurst_exponent=hurst,\n",
    "                volume_7d=volume_7d,\n",
    "                volume_7d_usd=volume_7d_usd,\n",
    "                last_update=datetime.now(),\n",
    "                ohlcv_data=df\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"‚úÖ {symbol} passed all filters: \"\n",
    "                       f\"Price=${current_price:.4f}, \"\n",
    "                       f\"Hurst={hurst:.4f}, \"\n",
    "                       f\"7d Volume=${volume_7d_usd:,.0f}\")\n",
    "            \n",
    "            return pair_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error screening {symbol}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def screen_all_pairs(self, max_workers: int = 3) -> List[PairData]:\n",
    "        \"\"\"Screen all USDT spot pairs with parallel processing and comprehensive error handling\"\"\"\n",
    "        try:\n",
    "            # Input validation\n",
    "            if max_workers <= 0 or max_workers > 10:\n",
    "                logger.warning(f\"Invalid max_workers: {max_workers}, using 3\")\n",
    "                max_workers = 3\n",
    "            \n",
    "            # Get all USDT pairs\n",
    "            pairs = self.data_fetcher.get_all_spot_usdt_pairs()\n",
    "            if not pairs:\n",
    "                logger.error(\"No pairs found to screen\")\n",
    "                return []\n",
    "            \n",
    "            # Limit pairs for performance (focus on top liquid pairs)\n",
    "            max_pairs = 50  # Screen top 50 most liquid pairs\n",
    "            if len(pairs) > max_pairs:\n",
    "                pairs = pairs[:max_pairs]\n",
    "                logger.info(f\"Limited screening to top {max_pairs} most liquid pairs\")\n",
    "            \n",
    "            logger.info(f\"Starting screening of {len(pairs)} pairs with {max_workers} workers...\")\n",
    "            \n",
    "            # Screen pairs in parallel\n",
    "            results = []\n",
    "            completed = 0\n",
    "            total = len(pairs)\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                # Submit all tasks\n",
    "                future_to_pair = {\n",
    "                    executor.submit(self.screen_single_pair, pair): pair \n",
    "                    for pair in pairs\n",
    "                }\n",
    "                \n",
    "                # Process completed tasks\n",
    "                for future in as_completed(future_to_pair):\n",
    "                    pair = future_to_pair[future]\n",
    "                    completed += 1\n",
    "                    \n",
    "                    try:\n",
    "                        result = future.result(timeout=30)  # 30 second timeout per pair\n",
    "                        if result:\n",
    "                            results.append(result)\n",
    "                            logger.info(f\"Progress: {completed}/{total} - Found {len(results)} valid pairs\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing {pair}: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Progress update every 10 pairs\n",
    "                    if completed % 10 == 0:\n",
    "                        logger.info(f\"Progress: {completed}/{total} pairs processed\")\n",
    "            \n",
    "            # Sort by volume (descending) and take top N\n",
    "            results.sort(key=lambda x: x.volume_7d_usd, reverse=True)\n",
    "            top_results = results[:self.criteria.top_n_pairs]\n",
    "            \n",
    "            self.results = top_results\n",
    "            logger.info(f\"Screening completed. {len(results)} pairs passed filters.\")\n",
    "            logger.info(f\"Top {len(top_results)} pairs selected.\")\n",
    "            \n",
    "            return top_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in screening process: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_screening_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of screening results\"\"\"\n",
    "        if not self.results:\n",
    "            return {\"message\": \"No results available\"}\n",
    "        \n",
    "        summary = {\n",
    "            \"total_pairs_screened\": len(self.results),\n",
    "            \"criteria\": {\n",
    "                \"min_price\": self.criteria.min_price,\n",
    "                \"min_volume_7d_usd\": self.criteria.min_volume_7d_usd,\n",
    "                \"max_hurst\": self.criteria.max_hurst,\n",
    "                \"years_of_data\": self.criteria.years_of_data\n",
    "            },\n",
    "            \"top_pairs\": []\n",
    "        }\n",
    "        \n",
    "        for i, pair in enumerate(self.results, 1):\n",
    "            summary[\"top_pairs\"].append({\n",
    "                \"rank\": i,\n",
    "                \"symbol\": pair.symbol,\n",
    "                \"close_price\": pair.close_price,\n",
    "                \"hurst_exponent\": pair.hurst_exponent,\n",
    "                \"volume_7d_usd\": pair.volume_7d_usd,\n",
    "                \"volume_7d\": pair.volume_7d\n",
    "            })\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_results(self, filename: str = \"screening_results.json\"):\n",
    "        \"\"\"Save screening results to file\"\"\"\n",
    "        try:\n",
    "            results_data = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"summary\": self.get_screening_summary(),\n",
    "                \"detailed_results\": []\n",
    "            }\n",
    "            \n",
    "            for pair in self.results:\n",
    "                results_data[\"detailed_results\"].append({\n",
    "                    \"symbol\": pair.symbol,\n",
    "                    \"close_price\": pair.close_price,\n",
    "                    \"hurst_exponent\": pair.hurst_exponent,\n",
    "                    \"volume_7d_usd\": pair.volume_7d_usd,\n",
    "                    \"volume_7d\": pair.volume_7d,\n",
    "                    \"last_update\": pair.last_update.isoformat()\n",
    "                })\n",
    "            \n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(results_data, f, indent=2, default=str)\n",
    "            \n",
    "            logger.info(f\"Results saved to {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving results: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the market screener - clean and simple\"\"\"\n",
    "    try:\n",
    "        logger.info(\"üöÄ Starting Market Screener for Grid Trading Pairs\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Load API credentials\n",
    "        api_key, api_secret = _load_api_credentials()\n",
    "        \n",
    "        # Initialize screener\n",
    "        screener = MarketScreener(api_key=api_key, api_secret=api_secret)\n",
    "        \n",
    "        # Run screening\n",
    "        start_time = time.time()\n",
    "        results = screener.screen_all_pairs(max_workers=3)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        # Display results\n",
    "        _display_results(results, duration)\n",
    "        \n",
    "        # Save results\n",
    "        screener.save_results()\n",
    "        \n",
    "        # Display summary\n",
    "        _display_summary(screener)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\n‚èπÔ∏è  Screening interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Fatal error in main: {e}\")\n",
    "        raise\n",
    "\n",
    "def _load_api_credentials() -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Load API credentials with error handling\"\"\"\n",
    "    try:\n",
    "        from api_config import get_api_credentials\n",
    "        api_key, api_secret = get_api_credentials()\n",
    "        \n",
    "        if not api_key or not api_secret:\n",
    "            logger.error(\"‚ùå API credentials not configured!\")\n",
    "            logger.info(\"Please update api_config.py with your Binance API credentials\")\n",
    "            return None, None\n",
    "        else:\n",
    "            logger.info(\"‚úÖ API credentials loaded successfully\")\n",
    "            return api_key, api_secret\n",
    "            \n",
    "    except ImportError:\n",
    "        logger.warning(\"‚ö†Ô∏è  api_config.py not found, running without API credentials\")\n",
    "        return None, None\n",
    "\n",
    "def _display_results(results: List[PairData], duration: float) -> None:\n",
    "    \"\"\"Display screening results\"\"\"\n",
    "    logger.info(f\"\\n‚è±Ô∏è  Screening completed in {duration:.2f} seconds\")\n",
    "    logger.info(f\"üìä Found {len(results)} pairs that passed all criteria\")\n",
    "    \n",
    "    if results:\n",
    "        logger.info(\"\\nüèÜ TOP PAIRS FOR GRID TRADING:\")\n",
    "        logger.info(\"-\" * 60)\n",
    "        \n",
    "        for i, pair in enumerate(results, 1):\n",
    "            logger.info(f\"{i}. {pair.symbol}\")\n",
    "            logger.info(f\"   üí∞ Price: ${pair.close_price:.4f}\")\n",
    "            logger.info(f\"   üìà Hurst: {pair.hurst_exponent:.4f}\")\n",
    "            logger.info(f\"   üíµ 7d Volume: ${pair.volume_7d_usd:,.0f}\")\n",
    "            logger.info(f\"   üìä 7d Volume (base): {pair.volume_7d:,.2f}\")\n",
    "            logger.info(\"\")\n",
    "    else:\n",
    "        logger.warning(\"‚ùå No pairs found that meet the criteria\")\n",
    "\n",
    "def _display_summary(screener: MarketScreener) -> None:\n",
    "    \"\"\"Display screening summary\"\"\"\n",
    "    summary = screener.get_screening_summary()\n",
    "    if \"criteria\" in summary:\n",
    "        logger.info(\"üìã SCREENING SUMMARY:\")\n",
    "        logger.info(f\"   Min Price: ${summary['criteria']['min_price']}\")\n",
    "        logger.info(f\"   Min 7d Volume: ${summary['criteria']['min_volume_7d_usd']:,.0f}\")\n",
    "        logger.info(f\"   Max Hurst: {summary['criteria']['max_hurst']}\")\n",
    "        logger.info(f\"   Data Period: {summary['criteria']['years_of_data']} years\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad62cc2-29fb-44ac-8428-3280df880ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:grid_trading]",
   "language": "python",
   "name": "conda-env-grid_trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
